public class Token_Stream
{
    public let source: Source_Text;
    public var position: int;
    public let diagnostics: mut system.collections.List<Diagnostic>;
    public var end_of_file: bool;

    public new(source: Source_Text)
    {
        self.source = source;
        self.position = 0; // TODO have to assign this zero for now because we don't have default values working
        self.diagnostics = new system.collections.List<Diagnostic>();
        self.end_of_file = false; // TODO implement auto initialize so this doesn't have to be initialized
    }
}

public next_token(tokens: mut Token_Stream) -> Token?
{
    if tokens.position >= source_byte_length(tokens.source)
    {
        return end_of_file_token(tokens);
    }

    var end: int = -1;
    while tokens.position < source_byte_length(tokens.source)
    {
        let cur_char: code_point = tokens.source.text[tokens.position];
        if cur_char == ' '
            or cur_char == '\t'
            or cur_char == '\n'
            or cur_char == '\r'
        {
            tokens.position += 1;
            continue;
        }
        else if cur_char == '{'
            { return new_operator_token(tokens, LeftBrace); }
        else if cur_char == '}'
            { return new_operator_token(tokens, RightBrace); }
        else if cur_char == '('
            { return new_operator_token(tokens, LeftParen); }
        else if cur_char == ')'
            { return new_operator_token(tokens, RightParen); }
        else if cur_char == ';'
            { return new_operator_token(tokens, Semicolon); }
        else if cur_char == ','
            { return new_operator_token(tokens, Comma); }
        else if cur_char == '.'
            { return new_operator_token(tokens, Dot); }
        else if cur_char == ':'
            { return new_operator_token(tokens, Colon); }
        else if cur_char == '['
            { return new_operator_token(tokens, LeftBracket); }
        else if cur_char == ']'
            { return new_operator_token(tokens, RightBracket); }
        else if cur_char == '?'
            { return new_operator_token(tokens, Question); }
        else if cur_char == '|'
            { return new_operator_token(tokens, Pipe); }
        else if cur_char == '*'
            { return new_operator_token(tokens, Asterisk); }
        else if cur_char == '='
        {
            if tokens.position + 1 < source_byte_length(tokens.source)
            {
                let next_char: code_point = tokens.source.text[tokens.position + 1];
                if next_char == '='
                {
                    // it is `==`
                    return new_operator_token(tokens, EqualsEquals, 2);
                }

                if next_char == '>'
                {
                    // it is `=>`
                    return new_operator_token(tokens, FatArrow, 2);
                }

                if tokens.position + 2 < source_byte_length(tokens.source)
                    and next_char == '/'
                    and tokens.source.text[tokens.position + 2] == '='
                {
                    // it is `=/=`
                    return new_operator_token(tokens, EqualsSlashEquals, 3);
                }
            }

            // it is `=`
            return new_operator_token(tokens, Equals);
        }
        else if cur_char == '+'
        {
            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '='
            {
                // it is `+=`
                return new_operator_token(tokens, PlusEquals, 2);
            }

            // it is `+`
            return new_operator_token(tokens, Plus);
        }
        else if cur_char == '-'
        {
            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '>'
            {
                // it is `->`
                return new_operator_token(tokens, Arrow, 2);
            }
            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '='
            {
                // it is `-=`
                return new_operator_token(tokens, MinusEquals, 2);
            }
            // it is `-`
            return new_operator_token(tokens, Minus);
        }
        else if cur_char == '/'
        {
            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '/'
            {
                // it is a line comment `//`
                while tokens.position < source_byte_length(tokens.source) and tokens.source.text[tokens.position] =/= '\r' and tokens.source.text[tokens.position] =/= '\n'
                {
                    tokens.position += 1;
                }

                continue;
            }

            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '*'
            {
                // it is a block comment `/*`
                tokens.position += 2; // move past the start
                var last_char_star: bool = false;
                while tokens.position < source_byte_length(tokens.source) and not (last_char_star and tokens.source.text[tokens.position] == '/')
                {
                    last_char_star = tokens.source.text[tokens.position] == '*';
                    tokens.position += 1;
                }

                tokens.position += 1; // move past the final '/'
                continue;
            }

            // it is `/`
            return new_operator_token(tokens, Slash);
        }
        else if cur_char == '%'
            { return new_operator_token(tokens, Percent); }
        else if cur_char == '<'
        {
            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '='
            {
                // it is `<=`
                return new_operator_token(tokens, LessThanEquals, 2);
            }
            // it is `<`
            return new_operator_token(tokens, LessThan);
        }
        else if cur_char == '>'
        {
            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '='
            {
                // it is `>=`
                return new_operator_token(tokens, GreaterThanEquals, 2);
            }
            // it is `>`
            return new_operator_token(tokens, GreaterThan);
        }
        else if cur_char == '"'
        {
            end = tokens.position + 1;
            var escaped: bool = false;
            while end < source_byte_length(tokens.source) and (tokens.source.text[end] =/= '"' or escaped)
            {
                escaped = tokens.source.text[end] == '\\' and not escaped;
                end += 1;
            }

            end += 1; // To include the close quote
            return new_token(tokens, StringLiteral, end);
        }
        else if cur_char == '\''
        {
            end = tokens.position + 1;
            var escaped: bool = false;
            while end < source_byte_length(tokens.source) and (tokens.source.text[end] =/= '\'' or escaped)
            {
                escaped = tokens.source.text[end] == '\\' and not escaped;
                end += 1;
            }

            end += 1; // To include the close quote
            return new_token(tokens, CodePointLiteral, end);
        }
        else
        {
            if is_identifier_char(cur_char)
            {
                end = tokens.position + 1;
                while is_identifier_char(tokens.source.text[end])
                {
                    end += 1;
                }
                return new_identifier_or_keyword_token(tokens, end);
            }
            if is_number_char(cur_char)
            {
                end = tokens.position + 1;
                while is_number_char(tokens.source.text[end])
                {
                    end += 1;
                }
                return new_token(tokens, Integer, end);
            }

            // Add an error to the next token
            var diagnostic_span: Text_Span = new Text_Span(tokens.position, 1);
            add_item(tokens.diagnostics, new Diagnostic(CompilationError, Lexing, tokens.source, diagnostic_span, "Invalid character `" + code_point_to_string(cur_char) + "`"));
            // Skip past the character and continue lexing
            tokens.position = end;
        }
    }

    return end_of_file_token(tokens);
}

public end_of_file_token(tokens: mut Token_Stream) -> Token?
{
    if tokens.end_of_file
        { return none; }

    // Mark that we have returned the end of file token
    tokens.end_of_file = true;
    // The end of file token provides something to attach any final errors to
    return new_token(tokens, EndOfFile, tokens.position);
}

public new_identifier_or_keyword_token(tokens: mut Token_Stream, end: int) -> Token
{
    let length: int = end-tokens.position;
    let value: string = substring(tokens.source.text, tokens.position, length);
    var kind: int;
    if value == "new"
        { kind = NewKeyword; }
    else if value == "not"
        { kind = NotOperator; }
    else if value == "null"
        { kind = NullReservedWord; }
    else if value == "self"
        { kind = SelfKeyword; }
    else if value == "true"
        { kind = TrueKeyword; }
    else if value == "false"
        { kind = FalseKeyword; }
    else if value == "mut"
        { kind = MutableKeyword; }
    else if value == "code_point"
        { kind = CodePoint; }
    else if value == "string"
        { kind = String; }
    else if value == "int"
        { kind = Int; }
    else if value == "bool"
        { kind = Bool; }
    else if value == "void"
        { kind = Void; }
    else if value == "uint"
        { kind = UnsignedInt; }
    else if value == "var"
        { kind = VarKeyword; }
    else if value == "and"
        { kind = AndKeyword; }
    else if value == "or"
        { kind = OrKeyword; }
    else if value == "return"
        { kind = ReturnKeyword; }
    else if value == "loop"
        { kind = LoopKeyword; }
    else if value == "while"
        { kind = WhileKeyword; }
    else if value == "for"
        { kind = ForKeyword; }
    else if value == "let"
        { kind = LetKeyword; }
    else if value == "in"
        { kind = InKeyword; }
    else if value == "do"
        { kind = DoKeyword; }
    else if value == "if"
        { kind = IfKeyword; }
    else if value == "else"
        { kind = ElseKeyword; }
    else if value == "break"
        { kind = BreakKeyword; }
    else if value == "continue"
        { kind = ContinueKeyword; }
    else if value == "private"
        { kind = PrivateKeyword; }
    else if value == "protected"
        { kind = ProtectedKeyword; }
    else if value == "public"
        { kind = PublicKeyword; }
    else if value == "internal"
        { kind = InternalKeyword; }
    else if value == "class"
        { kind = ClassKeyword; }
    else if value == "enum"
        { kind = EnumKeyword; }
    else if value == "struct"
        { kind = StructKeyword; }
    else if value == "none"
        { kind = NoneKeyword; }
    else if value == "match"
        { kind = MatchKeyword; }
    else
    {
        kind = Identifier;
        if string_byte_length(value) > 1 and value[string_byte_length(value)-1] == '_'
        {
            var diagnostic_span: Text_Span = new Text_Span(tokens.position, end - tokens.position);
            add_item(tokens.diagnostics, new Diagnostic(CompilationError, Lexing, tokens.source, diagnostic_span,
                "Identifiers ending with underscore are reserved `"+value+"`"));
        }
    }

    return new_token(tokens, kind, end);
}

public new_operator_token(tokens: mut Token_Stream, kind: int) -> Token
{
    return new_token(tokens, kind, tokens.position + 1);
}

public new_operator_token(tokens: mut Token_Stream, kind: int, length: int) -> Token
{
    return new_token(tokens, kind, tokens.position + length);
}

public new_token(tokens: mut Token_Stream, kind: int, end: int) -> Token
{
    let token_diagnostics: mut system.collections.List<Diagnostic> = new system.collections.List<Diagnostic>();
    for let diagnostic: Diagnostic in tokens.diagnostics
    {
        add_item(token_diagnostics, diagnostic);
    }
    clear_list(tokens.diagnostics);

    let token: Token = new Token(kind, tokens.source, tokens.position, end - tokens.position, token_diagnostics);

    tokens.position = end;
    return token;
}

public new_token(tokens: mut Token_Stream, kind: int, end: int, value: string) -> Token
{
    let token_diagnostics: mut system.collections.List<Diagnostic> = new system.collections.List<Diagnostic>();
    for let diagnostic: Diagnostic in tokens.diagnostics
    {
        add_item(token_diagnostics, diagnostic);
    }
    clear_list(tokens.diagnostics);

    let token: Token = new Token(kind, tokens.source, tokens.position, end - tokens.position, token_diagnostics, value);

    tokens.position = end;
    return token;
}

public is_identifier_char(c: code_point) -> bool
{
    return (c >= 'a' and c <= 'z') or (c >= 'A' and c <= 'Z') or c == '_';
}

public is_number_char(c: code_point) -> bool
{
    return c >= '0' and c <= '9';
}
