public class Token_Stream
{
    public let source: Source_Text;
    public var position: int;
    public let diagnostics: mut system.collections.List<Diagnostic>;
    public var end_of_file: bool;

    public new(source: Source_Text)
    {
        self.source = source;
        self.position = 0; // TODO have to assign this zero for now because we don't have default values working
        self.diagnostics = new system.collections.List<Diagnostic>();
        self.end_of_file = false; // TODO implement auto initialize so this doesn't have to be initialized
    }
}

public next_token(tokens: mut Token_Stream) -> Syntax_Node?
{
    if tokens.position >= source_byte_length(tokens.source)
    {
        return end_of_file_token(tokens);
    }

    var end: int = -1;
    while tokens.position < source_byte_length(tokens.source)
    {
        let curChar: code_point = tokens.source.text[tokens.position];
        if curChar == ' '
            or curChar == '\t'
            or curChar == '\n'
            or curChar == '\r'
        {
            tokens.position += 1;
            continue;
        }
        else if curChar == '{'
            { return new_operator_token(tokens, LeftBrace); }
        else if curChar == '}'
            { return new_operator_token(tokens, RightBrace); }
        else if curChar == '('
            { return new_operator_token(tokens, LeftParen); }
        else if curChar == ')'
            { return new_operator_token(tokens, RightParen); }
        else if curChar == ';'
            { return new_operator_token(tokens, Semicolon); }
        else if curChar == ','
            { return new_operator_token(tokens, Comma); }
        else if curChar == '.'
            { return new_operator_token(tokens, Dot); }
        else if curChar == ':'
            { return new_operator_token(tokens, Colon); }
        else if curChar == '['
            { return new_operator_token(tokens, LeftBracket); }
        else if curChar == ']'
            { return new_operator_token(tokens, RightBracket); }
        else if curChar == '?'
            { return new_operator_token(tokens, Question); }
        else if curChar == '|'
            { return new_operator_token(tokens, Pipe); }
        else if curChar == '*'
            { return new_operator_token(tokens, Asterisk); }
        else if curChar == '='
        {
            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '='
            {
                // it is `==`
                return new_operator_token(tokens, EqualsEquals, 2);
            }

            if tokens.position + 2 < source_byte_length(tokens.source)
                and tokens.source.text[tokens.position + 1] == '/'
                and tokens.source.text[tokens.position + 2] == '='
            {
                // it is `=/=`
                return new_operator_token(tokens, EqualsSlashEquals, 3);
            }

            // it is `=`
            return new_operator_token(tokens, Equals);
        }
        else if curChar == '+'
        {
            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '='
            {
                // it is `+=`
                return new_operator_token(tokens, PlusEquals, 2);
            }

            // it is `+`
            return new_operator_token(tokens, Plus);
        }
        else if curChar == '-'
        {
            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '>'
            {
                // it is `->`
                return new_operator_token(tokens, Arrow, 2);
            }
            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '='
            {
                // it is `-=`
                return new_operator_token(tokens, MinusEquals, 2);
            }
            // it is `-`
            return new_operator_token(tokens, Minus);
        }
        else if curChar == '/'
        {
            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '/'
            {
                // it is a line comment `//`
                while tokens.position < source_byte_length(tokens.source) and tokens.source.text[tokens.position] =/= '\r' and tokens.source.text[tokens.position] =/= '\n'
                {
                    tokens.position += 1;
                }

                continue;
            }

            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '*'
            {
                // it is a block comment `/*`
                tokens.position += 2; // move past the start
                var lastCharStar: bool = false;
                while tokens.position < source_byte_length(tokens.source) and not (lastCharStar and tokens.source.text[tokens.position] == '/')
                {
                    lastCharStar = tokens.source.text[tokens.position] == '*';
                    tokens.position += 1;
                }

                tokens.position += 1; // move past the final '/'
                continue;
            }

            // it is `/`
            return new_operator_token(tokens, Slash);
        }
        else if curChar == '%'
            { return new_operator_token(tokens, Percent); }
        else if curChar == '<'
        {
            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '='
            {
                // it is `<=`
                return new_operator_token(tokens, LessThanEquals, 2);
            }
            // it is `<`
            return new_operator_token(tokens, LessThan);
        }
        else if curChar == '>'
        {
            if tokens.position + 1 < source_byte_length(tokens.source) and tokens.source.text[tokens.position + 1] == '='
            {
                // it is `>=`
                return new_operator_token(tokens, GreaterThanEquals, 2);
            }
            // it is `>`
            return new_operator_token(tokens, GreaterThan);
        }
        else if curChar == '"'
        {
            end = tokens.position + 1;
            var escaped: bool = false;
            while end < source_byte_length(tokens.source) and (tokens.source.text[end] =/= '"' or escaped)
            {
                escaped = tokens.source.text[end] == '\\' and not escaped;
                end += 1;
            }

            end += 1; // To include the close quote
            return new_token(tokens, StringLiteral, end);
        }
        else if curChar == '\''
        {
            end = tokens.position + 1;
            var escaped: bool = false;
            while end < source_byte_length(tokens.source) and (tokens.source.text[end] =/= '\'' or escaped)
            {
                escaped = tokens.source.text[end] == '\\' and not escaped;
                end += 1;
            }

            end += 1; // To include the close quote
            return new_token(tokens, CodePointLiteral, end);
        }
        else
        {
            if is_identifier_char(curChar)
            {
                end = tokens.position + 1;
                while is_identifier_char(tokens.source.text[end])
                {
                    end += 1;
                }
                return new_identifier_or_keyword_token(tokens, end);
            }
            if is_number_char(curChar)
            {
                end = tokens.position + 1;
                while is_number_char(tokens.source.text[end])
                {
                    end += 1;
                }
                return new_token(tokens, Number, end);
            }

            // Add an error to the next token
            var diagnosticSpan: Text_Span = new Text_Span(tokens.position, 1);
            tokens.diagnostics.add(new Diagnostic(CompilationError, Lexing, tokens.source, diagnosticSpan, "Invalid character `" + curChar + "`"));
            // Skip past the character and continue lexing
            tokens.position = end;
        }
    }

    return end_of_file_token(tokens);
}

public end_of_file_token(tokens: mut Token_Stream) -> Syntax_Node?
{
    if tokens.end_of_file
        { return none; }

    // Mark that we have returned the end of file token
    tokens.end_of_file = true;
    // The end of file token provides something to attach any final errors to
    return new_token(tokens, EndOfFileToken, tokens.position);
}

public new_identifier_or_keyword_token(tokens: mut Token_Stream, end: int) -> Syntax_Node
{
    let length: int = end-tokens.position;
    let value: string = tokens.source.text.Substring(tokens.position, length);
    var type: int;
    if value == "new"
        { type = NewKeyword; }
    else if value == "not"
        { type = NotOperator; }
    else if value == "null"
        { type = NullReservedWord; }
    else if value == "self"
        { type = SelfKeyword; }
    else if value == "true"
        { type = TrueKeyword; }
    else if value == "false"
        { type = FalseKeyword; }
    else if value == "mut"
        { type = MutableKeyword; }
    else if value == "code_point"
        { type = CodePoint; }
    else if value == "string"
        { type = String; }
    else if value == "int"
        { type = Int; }
    else if value == "bool"
        { type = Bool; }
    else if value == "void"
        { type = Void; }
    else if value == "uint"
        { type = UnsignedInt; }
    else if value == "var"
        { type = VarKeyword; }
    else if value == "and"
        { type = AndKeyword; }
    else if value == "or"
        { type = OrKeyword; }
    else if value == "return"
        { type = ReturnKeyword; }
    else if value == "loop"
        { type = LoopKeyword; }
    else if value == "while"
        { type = WhileKeyword; }
    else if value == "for"
        { type = ForKeyword; }
    else if value == "let"
        { type = LetKeyword; }
    else if value == "in"
        { type = InKeyword; }
    else if value == "do"
        { type = DoKeyword; }
    else if value == "if"
        { type = IfKeyword; }
    else if value == "else"
        { type = ElseKeyword; }
    else if value == "break"
        { type = BreakKeyword; }
    else if value == "continue"
        { type = ContinueKeyword; }
    else if value == "private"
        { type = PrivateKeyword; }
    else if value == "protected"
        { type = ProtectedKeyword; }
    else if value == "public"
        { type = PublicKeyword; }
    else if value == "internal"
        { type = InternalKeyword; }
    else if value == "class"
        { type = ClassKeyword; }
    else if value == "enum"
        { type = EnumKeyword; }
    else if value == "struct"
        { type = StructKeyword; }
    else if value == "none"
        { type = NoneKeyword; }
    else
    {
        type = Identifier;
        if value.byte_length > 1 and value[value.byte_length-1] == '_'
        {
            var diagnosticSpan: Text_Span = new Text_Span(tokens.position, end - tokens.position);
            tokens.diagnostics.add(new Diagnostic(CompilationError, Lexing, tokens.source, diagnosticSpan,
                "Identifiers ending with underscore are reserved `"+value+"`"));
        }
    }

    return new_token(tokens, type, end);
}

public new_operator_token(tokens: mut Token_Stream, type: int) -> Syntax_Node
{
    return new_token(tokens, type, tokens.position + 1);
}

public new_operator_token(tokens: mut Token_Stream, type: int, length: int) -> Syntax_Node
{
    return new_token(tokens, type, tokens.position + length);
}

public new_token(tokens: mut Token_Stream, type: int, end: int) -> Syntax_Node
{
    let token: mut Syntax_Node = new Syntax_Node(type, tokens.source, tokens.position, end - tokens.position);

    for let diagnostic: Diagnostic in tokens.diagnostics
    {
        add(token, diagnostic);
    }
    tokens.diagnostics.clear();

    tokens.position = end;
    return token;
}

public is_identifier_char(c: code_point) -> bool
{
    return (c >= 'a' and c <= 'z') or (c >= 'A' and c <= 'Z') or c == '_';
}

public is_number_char(c: code_point) -> bool
{
    return c >= '0' and c <= '9';
}
